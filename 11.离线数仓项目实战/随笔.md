在讲到新增会员的时候，创建dws.dws_member_add_day的表的时候，不使用分区的原因是因为数据少，但是ads.ads_new_member_cnt的汇总表数据更少，为什么还要使用分区表啊？创建分区的依据到底是什么？是数据量？还是其他？



修改Flume滚动文件的大小的意义？



创建表的时候，既有内部表，又有外部表，到底是什么时候用内部表，什么时候用外部表？还有就是表的存储格式，什么时候用默认的，什么时候用parquet



Hive使用MR引擎的时候，会存在shuffle，那么使用Tez引擎的时候，还会不会存在Shuffle，在那个阶段？



在做会员活跃度分析的时候，SQL报错，能不能给讲讲到底因为什么报错，感觉课上一笔带过，就说Hive不稳定要改写SQL，能不能讲讲为什么会这样，是什么原因导致的？还有就是 where 和 join的使用，在hive中，是应该先join 在使用where。还是先使用where 然后在对结果进行join？有什么区别吗？



在HDFS里面，文件以块的形式存储，默认是128M，Flume滚动设置为1个G的时候，存在HDFS里面的时候，一个block块是128M还是1个G？当数据比较多的时候，为什么Flume滚动设置比较大， 为什么？如何达到的优化？



如果HDFS的block是128M，Hive的默认设置mapred.max.split.size 约等于256M，此时会不会出现移动数据的现象？在学习MapReduce的时候，不是说尽量减少移动数据吗？



在执行活跃会员的脚本的时候，为什么只有三个map？而dwd处理ods的时候是14个map，3.5G的数据。数据进入到dwd层，数据被压缩到了多少？能不能给看看HDFS里面，各个表对应的数据存储文件的情况？



hive在读取文件的时候，在Reduce阶段，会不会合并小文件。parquet格式的是压缩文件，这个格式的文件需要不需要合并？





分区表  ==》put hdfs==>alter table 加载数据，select  count(*) 返回0，问什么？即 ods.ods_start_log 表

~~~sql
hdfs dfs -mkdir -p  /user/hive/warehouse/mydb.db/t1/dt=20200901
hdfs dfs -put t1.dat  /user/hive/warehouse/mydb.db/t1/dt=20200901

create table t1(c1 string) partitioned by (dt string);
alter table t1 add partition(dt=20200901);

select * from t1;					-- 可以正常看见数据
select count(*) from t1;    		-- 查询结果为0
select count(*) from t1 limit 1; 	-- 启动MR查询结果正确

-- 执行分析
ANALYZE TABLE t1 partition(dt=20200901) COMPUTE STATISTICS;

select * from t1;			-- 可以正常看见数据
select count(*) from t1;    -- 查询结果正常

原因：
hive (mydb)> explain select count(*) from t1 ;
OK
Explain
STAGE DEPENDENCIES:
  Stage-0 is a root stage

STAGE PLANS:
  Stage: Stage-0
    Fetch Operator
      limit: 1
      Processor Tree:
        ListSink

    
从执行计划中，可以看见此时没有启动MR。此时的结果是直接从MetaStore中获取。    
    
hive (test)> explain select count(*) from t1 limit 1;
OK
Explain
Plan optimized by CBO.

Vertex dependency in root stage
Reducer 2 <- Map 1 (CUSTOM_SIMPLE_EDGE)

Stage-0
  Fetch Operator
    limit:1
    Stage-1
      Reducer 2
      File Output Operator [FS_7]
        Limit [LIM_6] (rows=1 width=8)
          Number of rows:1
          Group By Operator [GBY_4] (rows=1 width=8)
            Output:["_col0"],aggregations:["count(VALUE._col0)"]
          <-Map 1 [CUSTOM_SIMPLE_EDGE]
            PARTITION_ONLY_SHUFFLE [RS_3]
              Group By Operator [GBY_2] (rows=1 width=8)
                Output:["_col0"],aggregations:["count()"]
                Select Operator [SEL_1] (rows=20 width=8)
                  TableScan [TS_0] (rows=20 width=8)
                    test@t1,t1,Tbl:COMPLETE,Col:NONE    
    
从执行计划中，可以看见此时启动MR，能获得正常的查询结果。

~~~

select * from TBLS where TBL_NAME='t1';



mysql> select * from PARTITION_PARAMS;
+---------+-----------------------+------------------------+
| PART_ID | PARAM_KEY             | PARAM_VALUE            |
+---------+-----------------------+------------------------+
|       2 | COLUMN_STATS_ACCURATE | {"BASIC_STATS":"true"} |
|       2 | numFiles              | 0                      |
|       2 | numRows               | 0                      |
|       2 | rawDataSize           | 0                      |
|       2 | totalSize             | 0                      |
|       2 | transient_lastDdlTime | 1599462545             |
+---------+-----------------------+------------------------+

####  

select * from PARTITION_PARAMS where part_id=2 and param_key='numRows';

update PARTITION_PARAMS set PARAM_VALUE=888 where part_id=2 and param_key='numRows';

update PARTITION_PARAMS set PARAM_VALUE=6666 where part_id=2 and param_key='numRows';

hive (mydb)> select count(*) from t1 ;
OK
_c0
888









当Flume采集的日志文件特别大的时候，可以把滚动的大小调的大一点，由于课上的数据比较小，所以滚动值设置的比较小，实际应该根据真实情况设计滚动文件的大小





大数据服务希望的多块物理磁盘，Hadoop的瓶颈不是CPU和内存，而是磁盘IO